{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's in a name?\n",
    "\n",
    "### A tutorial on my machine-learning workflow for predicting whether or not a post will be popular!\n",
    "----------------------------------------------------------------------------------------------------\n",
    "The purpose of the notebook is to describe my efforts to predict whether or not a post to the /r/datascience subreddit will be a success.  I define success as receiving more than the average number of votes.  What's unique about my methodology is that the prediction is based solely on the title of the redditor's post, hence the blog title: What's in a name?.  \n",
    "\n",
    "### Caveats\n",
    "Yes, I do know that day and time are powerful predictors of votes (likes,retweets) but I didn't want to confine/limit my prediction to these temporal parameters.  I am also aware that the predictive power of this model is barely good, thus, suggested improvements would be greatly appreciated.  I must remind you that this is my first dive into NLP and I might have made careless mistakes and overlooked obvious problems or solutions. Please be kind in your criticism.\n",
    "\n",
    "### Tools for Model Building and Visualization:\n",
    "For visualization, I am excitedly using a packaged called [Yellowbrick](http://www.scikit-yb.org/en/latest/).  Yellowbrick rings true the phrase, a picture is worth a thousand words.  Yellowbrick's visuals can help you make quick interpretations of the performance of your machine learning model.  For model buidling, I am using the well-known [Sci-kit learn](http://scikit-learn.org/stable/).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It always begins with imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from feature_extraction import Blob, Words, Exclude, WordCount, POS, Readable\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, SelectFromModel\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "\n",
    "from calzone import describe, lemmatize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can install all packages above: 'pip install -r [requirements.txt](requirements.txt)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps 1 - 4 (tldr)\n",
    "1.  Reads in csv-formatted subreddit data.  \n",
    "    1a. You can use the grab_posts function within calzone.py to collect posts inline\n",
    "2.  Optional: Data manipulation  \n",
    "    2a. Obtain normal distribution of votes by applying log transformation.  \n",
    "    2b. Remove outliers using z-scores greater than -2.5 and less than 2.5\n",
    "3.  Create labeled data (It is the boolean set to whether or not a title will be popular )\n",
    "4.  Split data into training and test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The First Steps!\n",
    "The first step of predicting the success of a post was grabbing [/r/datascience](https://www.reddit.com/r/datascience) submissions.  I provide several years' worth of [data](processed_datascience.csv.bz2) below, however, you can grab your own data by using the script [calzone.py](calzone.py) \n",
    "\n",
    "The second step was to clean up the data.  This can include removing characters or changing the case of words in the titles, but most importantly, it included dealing specifically with the number of votes that each title received.  We noticed that there were votes that were wildly different than the rest.  To handle these anomalies, we removed the outliers based on z-scores.  We then log transformed the votes in order to obtain a normal distribution.  In the histogram below, you can see that without normalization the number of votes is left-skewed.\n",
    "\n",
    "Thirdly, we specified the target.  The target is the class that is being predicted.  In our case, we are trying to predict whether or not a post receives greater than the average number of votes (SUCCESS!).  For this dataset, the average is 7.5 votes.\n",
    "\n",
    "Fourthly, we split the data into training and test sets.\n",
    "\n",
    "#### *** Side note ***\n",
    "* Use: \"python calzone.py -h\" for explanation of script commands. \n",
    "* Edit:  For calzone.py to work we use Reddit python API wrapper (PRAW).  You must supply PRAW with your own Reddit config parameters, which are obtained via Reddit developer account.  Your parameters must be set as environmental variables within calzone.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE VOTES:  7.5\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('processed_datascience.csv.bz2')\n",
    "\n",
    "# You can use the grab_posts module if you want to grab posts from specific subreddits and time-frames\n",
    "# To retrieve your own posts requires a reddit developer account\n",
    "#from calzone import grab_posts\n",
    "#data = grab_posts(sub='todayilearned', start='01/24/2018', end='02/05/2018', number=30000, verbose=True)\n",
    "\n",
    "# Data clean - \n",
    "# 1. remove '/' character\n",
    "# 2. lowercase all letters\n",
    "# 3. lemmatize all words - change words to their root words, ie 'wanted -> want'\n",
    "\n",
    "data['title'] = lemmatize(data)\n",
    "\n",
    "## Optional: Throw out outliers by including rows with Z-Scores less than 2.5 and greater than -2.5\n",
    "data['z_scores'] = np.abs((data.ups-data.ups.mean())/data.ups.std())\n",
    "data = data[data['z_scores']<= 2.5]\n",
    "\n",
    "## Optional: Log transformation of up-votes\n",
    "data['log_ups'] = np.log1p(data['ups'])\n",
    "\n",
    "# Create Label column defining whether or not the article's votes exceed the average vote for the subreddit\n",
    "data['gtavg'] = data.log_ups > data.log_ups.mean()\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(data.title, \n",
    "                                                    data.gtavg, \n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=25)\n",
    "\n",
    "print('AVERAGE VOTES: ', '{:03.1f}'.format(data.ups.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at the distribution of Votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![UpVote Distribution](images/upvote_dist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of Data\n",
    "As discussed in the introduction, we are building a machine learning model, so that we can simply use the title of the post to predict its success. Our data covers the past 3 years of the /r/datascience subreddit and it contains almost 9000 titles. \n",
    "    \n",
    "Below are the features/attributes of the titles that we're analyzing for feature selection.  This is the process by which we determine which attributes are the most informative.  In the table below, we can see that the title of the posts that we considered to be a success are slightly higher across most features.  The most interesting of these features for me were subjectivity, polarity, and noun_phrases, because of the variable correlation coefficients (see matrix below).  \n",
    "    \n",
    "The table below segments attributes for:  ** Please note that the table is a summary and shows the average values of each feature\n",
    "    1.  All Titles\n",
    "    2.  Successful Titles (greater than 7 votes)\n",
    "    3.  Unsuccessful Titles (less than or equal to 7 votes)\n",
    "\n",
    "The measured featuress are:  ** The table below shows the average of these features.\n",
    "    \n",
    "    1.  The numbers characters in the titles\n",
    "    2.  The number of words in the titles\n",
    "    3.  The number of noun phrases in the titles\n",
    "    4.  The subjectivity of the titles - ranging from 0(objective) to 1(subjective)\n",
    "    5.  The polarity of the titles: -1(negative) to 0(neutral) to 1(positive)\n",
    "    6.  The number of votes per title\n",
    "    7.  The number of consonants in titles\n",
    "    8.  The number of vowels in titles\n",
    "    9.  The number of syllables in titles\n",
    "    10. The kincaid score of each title (measures the reading grade level required to read the title)\n",
    "    11. The flesch score of each titles (measures how easy it is to read the title)\n",
    "    12. The parts-of-speech(pos) present in each title ( There are 35 pos thus this info isn't included in the table below) \n",
    "    \n",
    "#### *** Side Note ***\n",
    "Subjectivity is measured on a scale (0 to 1).  Zero is objective (expressing facts) or one is subjective (expressing opinion).\n",
    "\n",
    "Polarity is measured on a scale (-1 to 1).  Noting how positive or negative a title is; i.e. happy versus sad.\n",
    "\n",
    "#### Interesting Tidbits\n",
    "1. The most characters found in a title is 299. Wow!\n",
    "2. The most words found in a title is 58. Double Wow!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Titles in Dataset: 8662\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Characters</th>\n",
       "      <th>Words</th>\n",
       "      <th>Noun_Phrases</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Consonants</th>\n",
       "      <th>Vowels</th>\n",
       "      <th>Kincaid</th>\n",
       "      <th>Flesch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>57</td>\n",
       "      <td>10</td>\n",
       "      <td>1.472985</td>\n",
       "      <td>16</td>\n",
       "      <td>0.196596</td>\n",
       "      <td>0.074589</td>\n",
       "      <td>7.520319</td>\n",
       "      <td>39</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Success</th>\n",
       "      <td>60</td>\n",
       "      <td>11</td>\n",
       "      <td>1.535070</td>\n",
       "      <td>17</td>\n",
       "      <td>0.216157</td>\n",
       "      <td>0.088646</td>\n",
       "      <td>19.933066</td>\n",
       "      <td>41</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Failure</th>\n",
       "      <td>56</td>\n",
       "      <td>10</td>\n",
       "      <td>1.447868</td>\n",
       "      <td>16</td>\n",
       "      <td>0.188682</td>\n",
       "      <td>0.068902</td>\n",
       "      <td>2.498460</td>\n",
       "      <td>38</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Characters  Words  Noun_Phrases  Syllables  Subjectivity  Polarity  \\\n",
       "All              57     10      1.472985         16      0.196596  0.074589   \n",
       "Success          60     11      1.535070         17      0.216157  0.088646   \n",
       "Failure          56     10      1.447868         16      0.188682  0.068902   \n",
       "\n",
       "             Votes  Consonants  Vowels  Kincaid  Flesch  \n",
       "All       7.520319          39      18        8      47  \n",
       "Success  19.933066          41      19        8      48  \n",
       "Failure   2.498460          38      18        8      46  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Total Number of Titles in Dataset:', len(data['title']))\n",
    "results = describe(data) # calzone function to extract title attributes\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *** SPECIAL NOTE: Feature Analysis ***\n",
    "1.  Colinearity can be useful in determining which features to include in analysis (see [here](feature_analysis.ipynb)).\n",
    "    1. The idea is to remove features that highly correlate with each other.  \n",
    "    2. We used Yellowbrick to generate the correlation graph below.  The graph makes sense, there should be a correlation between vowels and syllables, as the number of vowels in a word typically determine the number of syllables.\n",
    "    3. More than likely the feature selection algorithm that we will use later will exclude vowels, consonants, and charcount features  .\n",
    "\n",
    "\n",
    "<img align='left', src=\"images/pearson1.png\", alt='Pearson Correlation Matrix'>\n",
    "![Pearson Correlation Matrix](images/pearson1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribution of words (unigrams/bigrams)\n",
    "We also needed to examine the words contained in the titles.  Again, we used Yellowbrick to generate a frequency distribution chart that showcases the most common terms.  As expected, since we are in /r/datascience, the term data scientist is at the top of the word list.  Most interesting are the themes that become apparent:\n",
    "1.  Positions (data scientist)\n",
    "2.  Tools & Topics (big data, python, machine learning, etc)\n",
    "3.  Employment (Career, experience, job, work)\n",
    "4.  Education ( learning, course, masters)\n",
    "\n",
    "<img src=\"images/freqdist.png\", alt='Frequency Distribution', align='left'>\n",
    "![Frequency Distribution](images/freqdist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5-7 (tldr):  \n",
    "\n",
    "  5.  Feature Extraction (see above note on colinearity)\n",
    "    \n",
    "    1.  pos:\n",
    "            a count of 35 different parts of speech in each title\n",
    "    2.  read:\n",
    "            scores related to readability of titles\n",
    "              1. Flesch_kincaid_grade: the grade level required to read the title\n",
    "              2. Syllable count: the number of syllables in the title\n",
    "              3. Flesch_reading_ease: score of how easy it is to read the title\n",
    "    3.  words:\n",
    "            any number of the following:\n",
    "              1. Consonants - the number of consonants in each title\n",
    "              2. Vowels - the number of vowels in each title\n",
    "              3. Words - the number of words in each title\n",
    "              4. Characters - the number of characters in each title\n",
    "    4.  blob:\n",
    "            the following characteristics of each title\n",
    "              1. Noun Phrases - the number of noun phrases in each title\n",
    "              2. Subjectivity - objectivity vesus Subjectivity (range 0 to 1)\n",
    "              3. Polarity - negative(-1) to positive(1) sentiment\n",
    "    5.  title:\n",
    "            vectorizes text, decomposes vector, and select features\n",
    "              1. Tf-idf Vectorizer\n",
    "                  A. Creates a word vector \n",
    "                      1. Word is weighted by its occurence within the corpus\n",
    "                  B.  Use bi-grams, stop_words, and sublinear term frequency\n",
    "              2.  TruncatedSVD reduces features through LSA decomposition\n",
    "              3.  SelectPercentile selects the top 10% of features\n",
    "\n",
    "  6.  Modeling - Classifier: RandomForest\n",
    "    1.  We chose this classifier for its speed.  \n",
    "    2.  We examined other classifiers but the results were pretty much the same.  \n",
    "    3.  We generated a prediction model by fitting on training data.\n",
    "    4.  We used the model to predict the test data labels (successful/unsuccessful).\n",
    "\n",
    "  7.  Model Evaluation\n",
    "    1. Preliminary Evaluation\n",
    "          1.  Measure prediction accuracy\n",
    "          2.  Cross validate (measure the predictive performance of a statistical model)\n",
    "          3.  Measure F1 Score\n",
    "    2. Visual Evaluation\n",
    "        \n",
    "  8.  Model Improvements\n",
    "    1. Improved by tuning hyperparameters, see [here](hyperparameters.ipynb).\n",
    "          1.  However in this instance, tuning only resulted in slight improve (1%).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction, Feature Selection and Modeling\n",
    "The titles provide us with two different types of information.  The first type is the general attributes of the title; i.e. how many words, syllables, vowels, etc... (see above for list of attributes) in each title.  The second type is the words themselves.  The question is how do we use both types of information to predict if a title will be successful.  For the attributes, it is simple because they are numbers; it could be as easy as finding out which combination of attributes define success versus failure.  How to define success with just words becomes tricky.  Essentially, performing machine learning on the titles requires that the text content be turned into numerical feature vectors (feature extraction).  \n",
    "\n",
    "For feature extraction on the titles, we used a count vectorizer that measures term frequency(tf); i.e. how often a word appears in a title.  For instance, if we do this for the following sentences, then we produce the matrix below.  \n",
    "\n",
    "\n",
    "#### Title A: The dog jumped over the fence\n",
    "#### Title B: The cat chased the dog\n",
    "#### Title C: The white cat chased the brown cat who jumped over the orange cat\n",
    "\n",
    "\n",
    "|          |the | dog | jumped | over | fence | cat | chased | white | brown | who | orange|\n",
    "|:----------------------------------------------------------------------------------------:|\n",
    "|Title A   | 2  |  1  |  1     |  1   |   1   |  0  |   0    |   0   |   0   |  0  |   0   |\n",
    "|Title B   | 1  |  1  |  0     |  0   |   0   |  1  |   1    |   0   |   0   |  0  |   0   |\n",
    "|Title C   | 3  |  0   | 1     |  1   |   0   |  3  |   1    |   1   |   1   |  1  |   1   |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The downside of just using tf is that words that appear most often tend to dominate the matrix.  To overcome this, we used the product of term frequency and inverse document frequency (tf-idf).  Idf is the measure of whether a term is common or rare across all documents [Side note 2]. \n",
    "\n",
    "Essentially, tf-idf creates a word vector in which a word is weighted by its occurence not only in the title it was derived from, but also the corpus (the entire group of titles). A complete example on how tf-idf is calculated can be found [here](tfidf.ipynb).\n",
    "\n",
    "The tf-idf process left us with about a vocabulary of 58,000 (see side note, for explanation of why this is so large).  Not all these words/phrases are going to provide useful information.  The trick is to find a reduced dimensional representation of our matrix that emphasizes the strongest relationships and throws away the noise.  To achieve this, we used a dimension reduction technique called TruncatedSVD aka Latent Semantic Analysis[(LSA)](https://technowiki.wordpress.com/2011/08/27/latent-semantic-analysis-lsa-tutorial/).  After reducing our dimensions with LSA, we further compacted our matrix by using Anova F-Value feature selection.\n",
    "\n",
    "Let's not forget about the title attributes, they also need to undergo feature selection.  We did this by using tree-based estimators (ExtraTreesClassifier) to compute feature importance, which can be used to discard irrelevant features.  This resulted in a reduction from 45 to 17 most important attributes.\n",
    "\n",
    "Using Sci-kit Learn's \"feature union\" function, we can combined the attribute and tf-idf vectors, then inserted this vector into a Random Forest Classifier.  We chose Random Forest only after trying out several other classifers, such as Naive Bayes, Multilayer Perceptron, and K Nearest Neighbors.  We optimized our model by tuning the [hyperparameters](hyperparameters.ipynb).\n",
    "We evaluated our model with common scoring methods: our accuracy was 55%.  The charts below provide more visual evaluations.  A great model would have higher precision, recall, and F1 Score, plus a ROC curve that hugs the y-axis.  I do not have a great model and I'm ok with that!  The code for the visual evaluation charts is [here](model_evaluation.ipynb).  \n",
    " \n",
    " \n",
    " #### Side Note 1:\n",
    " To get even more information out of a title we use unigrams, bigrams and trigrams.  This is a group of one, two or three words.  For example, \"The boy ran\" would break down into 'the', 'boy', 'ran', 'the boy', 'boy ran', and 'the boy ran'.\n",
    " #### Side Note 2:\n",
    " tf-idf [reference](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datascience.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Sklearn has a pipeline Class that directs the flow of model creation; \n",
    "# below the pipeline corals the features into the Random Forest classifer.\n",
    "# 2. Within the pipeline is a sklearn Class called FeatureUnion.  \n",
    "# 3. Feature Union allows for the joining of multiple features into a single vector\n",
    "# 4. Within the feature union is a transformer list,\n",
    "# containingclasses that performed the functions described above\n",
    "# 5. The final pipeline item is the declaration of a classifier,\n",
    "# that the combined feature vector will be inserted into\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "                      \n",
    "            ('pipe', Pipeline([\n",
    "                ('inner', FeatureUnion(\n",
    "                    transformer_list=[\n",
    "                        ('pos', POS()),\n",
    "\n",
    "                        ('read', Readable()),\n",
    "\n",
    "                        ('words', Words()),\n",
    "\n",
    "                        ('blob', Pipeline([\n",
    "                            ('all', Blob()),\n",
    "                            ('minmax', MinMaxScaler()),\n",
    "                        ])),\n",
    "                ])),\n",
    "                ('select', SelectFromModel(ExtraTreesClassifier()))\n",
    "   \n",
    "            ])),\n",
    "                      \n",
    "            ('title', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b', \n",
    "                                          ngram_range=(1,3), sublinear_tf=True,\n",
    "                                          strip_accents='unicode', stop_words='english')),\n",
    "                ('svd', TruncatedSVD(n_components=120)),\n",
    "                ('normalize', MinMaxScaler(copy=False)),\n",
    "                ('selector', SelectPercentile(f_classif, percentile=10))\n",
    "            ])),\n",
    "\n",
    "            \n",
    "            ])),\n",
    "    ('clf', RandomForestClassifier(n_estimators=190, n_jobs=-1, max_depth=5, max_features='log2',\n",
    "                                  min_samples_leaf=1, min_samples_split=77)),\n",
    "        ])\n",
    "\n",
    "# Train model\n",
    "pipeline.fit(train_X, train_y)\n",
    "\n",
    "# Predict Test Set\n",
    "y_pred = pipeline.predict(test_X)\n",
    "\n",
    "# Save our model\n",
    "joblib.dump(pipeline, 'datascience.xz', compress=('xz', 9))\n",
    "\n",
    "# Test it out\n",
    "pipeline.predict(pd.Series(['A tutorial on my machine-learning workflow \\\n",
    "                            for predicting whether or not this post will be popular']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation - Preliminary Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.9%\n",
      "F1 Score: 0.538\n"
     ]
    }
   ],
   "source": [
    "# Measure prediction Accuracy and F1 Score\n",
    "accuracy = accuracy_score(y_pred=y_pred, y_true=test_y)\n",
    "print('Accuracy: {:03.1f}%'.format(accuracy * 100))\n",
    "\n",
    "print('F1 Score: {:.3f}'.format(f1_score(test_y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scores:  [ 0.56596972  0.52597403  0.56204906  0.57039711  0.56245487]\n"
     ]
    }
   ],
   "source": [
    "# Cross Validate prediction Score\n",
    "print('CV Scores: ', cross_val_score(pipeline, train_X, train_y, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation - Visual Evaluation\n",
    "In the graphs below, False or Failure equates to less than 7.5 votes while True or Success equates to greater than 7.5 votes.\n",
    "All the graphs were generated by Yellowbrick.\n",
    "    1.  Top left: Classification Report\n",
    "    2.  Top Right: Confusion Matrix\n",
    "    3.  Bottom Left: Class Balance\n",
    "    4.  Bottom Right: ROC-AUC Curve\n",
    "\n",
    "\n",
    "** Editorial Review - Ben: The below section is just a place holder, the text originated from yellowbrick visualizer pages **\n",
    "\n",
    "The classification report visualizer displays the precision, recall, and F1 scores for the model. The higher the score, the more predictive power your model contains.  My model has slightly over 50%, in other words, a toss of a coin.\n",
    "\n",
    "The confusion matrix shows how each of the test values predicted classes compare to their actual classes. Data scientists use confusion matrices to understand which classes are most easily confused.\n",
    "\n",
    "Oftentimes classifiers perform badly because of a class imbalance. A class balance chart can help prepare the user for such a case by showing the support for each class in the fitted classification model.  My classes were fairly balanced.\n",
    "\n",
    "A ROCAUC (Receiver Operating Characteristic/Area Under the Curve) plot allows the user to visualize the tradeoff between the classifier’s sensitivity and specificity.  You will notice that this model has neither sensitivity nor specificity.\n",
    "\n",
    "** End Editorial Review **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Classification Report          |Confusion Matrix               |\n",
    "|:-----------------------------:|:-----------------------------:|\n",
    "| <img src=\"images/clsreport1.png\"> | <img src=\"images/confusion.png\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Class Balance                 | ROC Curve                     |\n",
    "|:-----------------------------:|:-----------------------------:|\n",
    "| <img src=\"images/classbalance.png\"> | <img src=\"images/rocauc.png\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "My goal was to go through and document the entire process of building a classifier.  I learned a lot.  Now, I know that a Reddit title isn't sufficient enough to predict whether or not a post will be successful.  Getting to this conclusion after so many hours of work was heartbreaking but at the same time very fulfilling.  I hope you can take my failure and use it to build successful models.  Also, I hope you will return for my follow-up blog post on how to predict the number of votes a title will receive.  Until next time, Adios :)\n",
    "\n",
    "## Addendum - Experimentation\n",
    "The title of this blog was generated by trying different titles and seeing which title would be predicted to be a success.  So, next, I published this blog to r/datascience, r/machinelearning, and r/python subreddits.  The results after a couple of days were astonishing, the datascience subreddit post received 17 votes while machinelearning and python only got 0 and 1 votes respectively.  You might be confused by the zero votes but I think it worked as it was built for. The machine learning and python postings were meant as controls. You have to remember that our model was built specifically for r/datascience subreddit.  The 17 votes may not seem like a lot of votes, but you need to take into consideration that unpopular posts in /r/datascience receive on average 2 votes. \n",
    "\n",
    "### WooHoo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Editorial Review - Ben: Can I include this?  You can say No and I would completely understand **\n",
    "### P.S. I am looking to switch careers from computational biology to data science. Message me if you have any positions available.\n",
    "** End Editorial Review **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
